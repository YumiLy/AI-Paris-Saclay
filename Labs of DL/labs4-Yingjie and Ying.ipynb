{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f73c13a-6a41-41a8-b3cb-bf0aaac86ebc",
   "metadata": {},
   "source": [
    "# Deep Learning - Lab Exercise 2\n",
    "\n",
    "### Student 1: Ying LAI. Student 2: Yingjie LIU\n",
    "\n",
    "**WARNING:** you must have finished the previous exercise before this one as you will re-use parts of the code.\n",
    "\n",
    "In the first lab exercise, we built a simple linear classifier.\n",
    "Although it can give reasonable results on the MNIST datasetÂ (~92.5% of accuracy), deeper neural networks can achieve more the 99% accuracy.\n",
    "However, it can quickly become really impracical to explicitly code forward and backward passes.\n",
    "Hence, it is useful to rely on an auto-diff library where we specify the forward pass once, and the backward pass is automatically deduced from the computational graph structure.\n",
    "\n",
    "In this lab exercise, we will build a small and simple auto-diff lib that mimics the autograd mechanism from Pytorch (of course, we will simplify a lot!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576d2e25-2822-46e1-a6de-80f793898317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs that we will use\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# To load the data we will use the script of Gaetan Marceau Caron\n",
    "# You can download it from the course webiste and move it to the same directory that contains this ipynb file\n",
    "import dataset_loader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ce741f-b2a7-422d-96de-d71cb79cfdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mnist dataset \n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    # this link doesn't work any more,\n",
    "    # seach on google for the file \"mnist.pkl.gz\"\n",
    "    # and download it\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "# if you have it somewhere else, you can comment the lines above\n",
    "# and overwrite the path below\n",
    "mnist_path = \"./mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4204abd-5cce-48f1-a78f-392a8275fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 3 splits\n",
    "train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d35084-f93b-4245-b5cf-55baf1eb5f67",
   "metadata": {},
   "source": [
    "## Computation Graph\n",
    "\n",
    "Instead of directly manipulating numpy arrays, we will manipulate abstraction that contains:\n",
    "- a value (i.e. a numpy array)\n",
    "- a bool indicating if we wish to compute the gradient with respect to the value\n",
    "- the gradient with respect to the value\n",
    "- the operation to call during backpropagation\n",
    "\n",
    "There will be two kind of nodes:\n",
    "- ComputationGraphNode: a generic computation node\n",
    "- Parameter: a computation node that is used to store parameters of the network. Parameters are always leaf nodes, i.e. they cannot be build from other computation nodes.\n",
    "\n",
    "Our implementation of the backward pass will be really simple and incorrect in the general case (i.e. won't work with computation graph with loops).\n",
    "We will just apply the derivative function for a given tensor and then call the ones of its antecedents, recursively.\n",
    "This simple algorithm is good enough for this exercise.\n",
    "\n",
    "Note that a real implementation of backprop will store temporary values during forward that can be used during backward to improve computation speed. We do not do that here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1535415-f6ce-486a-850a-120cd9008642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationGraphNode(object):\n",
    "    \n",
    "    def __init__(self, data, require_grad=False):\n",
    "        # we initialise the value of the node and the grad\n",
    "        if(not isinstance(data, np.ndarray)):\n",
    "            data = np.array(data)\n",
    "        self.value = data\n",
    "        self.grad = None\n",
    "        \n",
    "        self.require_grad = require_grad\n",
    "        self.func = None\n",
    "        self.input_nodes = None\n",
    "        self.func_parameters = []\n",
    "    \n",
    "    def set_input_nodes(self, *nodes):\n",
    "        self.input_nodes = list(nodes)\n",
    "\n",
    "    def set_func_parameters(self, *func_parameters):\n",
    "        self.func_parameters = list(func_parameters)\n",
    "    \n",
    "    def set_func(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def zero_grad(self):\n",
    "        if self.grad is not None:\n",
    "            self.grad.fill(0)\n",
    "\n",
    "    def set_gradient(self, gradient):\n",
    "        \"\"\"\n",
    "        Accumulate gradient for this tensor\n",
    "        \"\"\"\n",
    "        if gradient.shape != self.value.shape:\n",
    "            print(gradient.shape, self.value.shape)\n",
    "            raise RuntimeError(\"Invalid gradient dimension\")\n",
    "        if self.grad is None:\n",
    "            self.grad = gradient\n",
    "        else:\n",
    "            self.grad += gradient\n",
    "    \n",
    "    def backward(self, g=None):\n",
    "        if g is None:\n",
    "            g = self.value.copy()\n",
    "            g.fill(1.)\n",
    "        self.set_gradient(g)\n",
    "        if self.func is not None:\n",
    "            grad_list = self.func.backward(*(self.input_nodes + self.func_parameters + [g]))\n",
    "            for input_node, ngrad in zip(self.input_nodes, grad_list):\n",
    "                input_node.backward(ngrad)\n",
    "    \n",
    "    def __add__(self, y):\n",
    "        if not isinstance(y, ComputationGraphNode):\n",
    "            y = ComputationGraphNode(y)\n",
    "        return Addition()(self, y)\n",
    "\n",
    "    def __getitem__(self, slice):\n",
    "        return Selection()(self, slice)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value.__str__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.value.__str__()\n",
    "\n",
    "class Parameter(ComputationGraphNode):\n",
    "    def __init__(self, data, name=\"default\"):\n",
    "        super().__init__(data, require_grad=True)\n",
    "        self.name  = name\n",
    "\n",
    "    def backward(self, g=None):\n",
    "        if g is not None:\n",
    "            self.set_gradient(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24dbd86-f592-4f8a-8657-272568a44f02",
   "metadata": {},
   "source": [
    "The class `Operation` is a class that three methods you should reimplement only the forward and the backward methods.\n",
    "* The `forward` method compute the function w.r.t inputs and return a new node that must contains information for backward pass.\n",
    "* The `backward` functions compute the gradient of the function w.r.t gradient of the output and other informations (forward pass input, parameter of the function...).**It should return a tuple**\n",
    "\n",
    "For better understanding below two operation are implemented, the selection and the addition (notice that it should not works yet since we do not defined what is a node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca2caa2d-da91-4765-b709-b31acad0c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    @staticmethod\n",
    "    def forward(*args):\n",
    "        raise NotImplementedError(\"It is an abstract method\")\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        output_node = self.forward(*args)\n",
    "        output_node.set_func(self)\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(*args):\n",
    "        pass\n",
    "class Addition(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, y):\n",
    "        output_array = x.value + y.value\n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "        output_node.set_input_nodes(x, y)\n",
    "        return output_node\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x, y, gradient):\n",
    "        return (gradient, gradient)\n",
    "\n",
    "class Selection(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, slice):\n",
    "        np_x = x.value\n",
    "\n",
    "        output_array = np_x.__getitem__(slice)\n",
    "        \n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "        output_node.set_input_nodes(x)\n",
    "        output_node.set_func_parameters(slice)\n",
    "\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x, slice, gradient):\n",
    "        np_x = x.value\n",
    "\n",
    "        cgrad = np_x.copy()\n",
    "        cgrad.fill(0)\n",
    "        cgrad.__setitem__(slice, gradient)\n",
    "        \n",
    "        return cgrad,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8a571-436a-499a-986a-f8a5ae89dab4",
   "metadata": {},
   "source": [
    "**Question 1** Complete the following class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a3f6684-1b56-40ad-8fe4-7dd9373f81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        # we copy the value of the input node\n",
    "        np_x = x.value.copy()\n",
    "\n",
    "        # set negative elements to zero\n",
    "        np_x[np_x < 0] = 0 # notice we consider strictly < 0 \n",
    "\n",
    "        # we create the output node needing only the node x\n",
    "        output_node = ComputationGraphNode(np_x)\n",
    "        output_node.set_input_nodes(x)\n",
    "        \n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x, gradient):\n",
    "        \n",
    "        # we copy the value of the input node\n",
    "        np_x = x.value.copy()\n",
    "\n",
    "        # np_x[np_x <= 0] = 0 # consider <= 0\n",
    "        # np_x[np_x > 0] = 1\n",
    "        # grad = gradient * np_x\n",
    "        # we multiply the gradient by the gradient of the ReLU\n",
    "        grad = (np_x > 0).astype(np_x.dtype) * gradient      \n",
    "\n",
    "        return (grad,)\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82289abf-c8b1-495f-86b7-e8e27cd0cac3",
   "metadata": {},
   "source": [
    "We recall that :  $$tanh(x)= \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$$ \n",
    "\n",
    "However we can have stability issues if $||z||$ is large, e.g. $e^{10000}$ will lead to computation error or infinity. Indeed in python using numpy:\n",
    "\n",
    "\n",
    ">np.exp(10000)\n",
    "\n",
    "\n",
    "will leads to :\n",
    "\n",
    ">/tmp/ipykernel_7784/2473798304.py:1: RuntimeWarning: overflow encountered in exp\n",
    ">np.exp(10000)\n",
    ">\n",
    ">inf\n",
    "\n",
    "We can use the same tricks that the one used in the softmax computation observing the simple following fact: \n",
    "$$\n",
    "\\begin{aligned}\n",
    " tanh(x) &= \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} \\\\\n",
    " &= \\left(\\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\\right)\\frac{e^{-a}}{e^{-a}} \\\\\n",
    " &= \\frac{e^{z}e^{-a} - e^{-z}e^{-a}}{e^{z}e^{-a} + e^{-z}e^{-a}} \\\\\n",
    "&= \\frac{e^{z-a} - e^{-z-a}}{e^{z-a} + e^{-z-a}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Thus we want that $z-a$ or $-z-a$ be small, or in our case lower than $0$.  Thus taking $a$ as the absolute value of $z$ ($|z|$) will leads to have \n",
    "$z-a\\leq 0$ and $-z-a\\leq 0$.\n",
    "\n",
    "\n",
    "For the backward notice that $tanh'(x) = 1-\\sigma(x)^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89adae37-5902-4e50-ac29-5ed332cc0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH(Operation):\n",
    "    @staticmethod\n",
    "    def TanHCompute(x):\n",
    "\n",
    "        \n",
    "        a = np.abs(x)\n",
    "        tanh =  (np.exp(x+a) - np.exp(-x+a)) / (np.exp(x+a) + np.exp(-x+a))\n",
    "        return tanh\n",
    "        # return np.tanh(x)\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        # we copy the value of the input node\n",
    "        np_x = x.value.copy()\n",
    "        \n",
    "        # we compute the tanh\n",
    "        output_value = TanH.TanHCompute(np_x)\n",
    "\n",
    "        # we create the output node needing only the node x\n",
    "        output_node = ComputationGraphNode(np_x)\n",
    "        output_node.set_input_nodes(x)\n",
    "\n",
    "        return output_node\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x, gradient):\n",
    "        # we copy the value of the input node\n",
    "        np_x = x.value.copy()\n",
    "\n",
    "        # we compute the gradient\n",
    "        grad = gradient * (1 - TanH.TanHCompute(np_x)**2)\n",
    "        return (grad,)\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d873cf-a10f-4e33-ac84-0018933ab4c5",
   "metadata": {},
   "source": [
    "**Question 2:** Next, we implement the affine transform operation.\n",
    "You can reuse the code from the third lab exercise, with one major difference: you have to compute the gradient with respect to x too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95cf9932-ddcf-472f-8732-c645b5063cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class affine_transform(Operation):\n",
    "    @staticmethod\n",
    "    def forward(W, b, x):\n",
    "        y = W.value @ x.value + b.value\n",
    "        output_node = ComputationGraphNode(y)\n",
    "        output_node.set_input_nodes(W, b, x)\n",
    "        return output_node\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(W, b, x, gradient):\n",
    "        \n",
    "        \n",
    "        # Calculate gradients for weights\n",
    "        if x.value.ndim > 1:\n",
    "            #\n",
    "            grad_W = gradient.T @ x.value / gradient.shape[0]\n",
    "            grad_x = gradient @ W.value\n",
    "        else:\n",
    "            grad_W = np.outer(gradient, x.value)\n",
    "            grad_x = W.value.T @ gradient\n",
    "    \n",
    "        \n",
    "    \n",
    "        # Calculate gradients for biases\n",
    "        grad_b = gradient.sum(axis=0) if gradient.ndim>1 else gradient\n",
    "        \n",
    "        return grad_W, grad_b, grad_x\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372bea6-9480-4c5c-b3c4-0bdc570abecb",
   "metadata": {},
   "source": [
    "**Question 3:** Define the NLL operation\n",
    "\n",
    "We recall that \n",
    "$$nll(x, y)= -log\\left(\\frac{e^{x_{y}}}{ \\sum\\limits_{i=1}^n e^{x_{ j}} }\\right) = -x_{y} + log(\\sum\\limits_{i=1}^n e^{x_{ j} })$$\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\frac{\\partial nll(x, y)}{\\partial x_i} &= - \\mathbb{1}_{y = i} + \\frac{\\partial log(\\sum\\limits_{i=1}^n e^{x_{ j} })}{\\partial\\sum\\limits_{i=1}^n e^{x_{ j} }}\\frac{\\sum\\limits_{i=1}^n e^{x_{ j} }}{\\partial x_i} \\\\\n",
    "        &= - \\mathbb{1}_{y = i} + \\frac{e^{x_i}}{\\sum\\limits_{i=1}^n e^{x_{ j} }} \n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3f742f0-83f3-4983-ae7a-cefd511b96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nll(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, y):\n",
    "        # Convert x and y to appropriate shapes\n",
    "        x_new = np.atleast_2d(x.value)\n",
    "        y_new = np.atleast_1d(y.value).astype(int)\n",
    "\n",
    "        # Apply softmax to input values\n",
    "        sf = nll.softmax(x_new, axis=1)\n",
    "    \n",
    "        # Select the probabilities of the correct classes\n",
    "        correct_class = sf[np.arange(len(sf)), y_new]\n",
    "    \n",
    "        # Calculate NLL loss\n",
    "        nll_loss = -np.log(correct_class)\n",
    "    \n",
    "        # Adjust the output based on whether we have a single sample or a batch\n",
    "        output_value = nll_loss[0] if len(nll_loss)==1 else nll_loss.mean()\n",
    "\n",
    "        # Create and configure the output node\n",
    "        output_node = ComputationGraphNode(output_value)\n",
    "        output_node.set_input_nodes(x)\n",
    "        output_node.set_func_parameters(y)\n",
    "        return output_node\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x, y, gradient):\n",
    "        x_new = np.atleast_2d(x.value)\n",
    "        y_new = np.atleast_1d(y.value).astype(int)\n",
    "        # Compute softmax predictions\n",
    "        sf = nll.softmax(x_new, axis=1)\n",
    "    \n",
    "        # Initialize gradient of x with softmax outputs\n",
    "        grad_x = sf.copy()\n",
    "    \n",
    "        # Subtract 1 from the gradient of the correct classes\n",
    "        grad_x[np.arange(len(sf)), y_new] -= 1\n",
    "    \n",
    "        # Adjust gradient for the number of samples\n",
    "        factor = 1 if x_new.shape[0] == 1 else x_new.shape[0]\n",
    "        grad_x *= (gradient / factor)\n",
    "        # grad_x *= (gradient / max(x_new.shape[0], 1))\n",
    "    \n",
    "        # If the original input was a 1D array, flatten the output gradient\n",
    "        if x.value.ndim == 1:\n",
    "            grad_x = grad_x.flatten()\n",
    "        return grad_x, None\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x, axis=None):\n",
    "        exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016217fb-a9f4-4908-b3e7-5fe372b38946",
   "metadata": {},
   "source": [
    "# Module\n",
    "\n",
    "Neural networks or parts of neural networks will be stored in Modules.\n",
    "They implement method to retrieve all parameters of the network and subnetwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43ea790d-a625-4aa4-95f1-8e4fe5e558f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        raise NotImplemented(\"\")\n",
    "        \n",
    "    def parameters(self):\n",
    "        ret = []\n",
    "        for name in dir(self):\n",
    "            o = self.__getattribute__(name)\n",
    "\n",
    "            if type(o) is Parameter:\n",
    "                ret.append(o)\n",
    "            if isinstance(o, Module) or isinstance(o, ModuleList):\n",
    "                ret.extend(o.parameters())\n",
    "        return ret\n",
    "\n",
    "# if you want to store a list of Parameters or Module,\n",
    "# you must store them in a ModuleList instead of a python list,\n",
    "# in order to collect the parameters correctly\n",
    "class ModuleList(list):\n",
    "    def parameters(self):\n",
    "        ret = []\n",
    "        for m in self:\n",
    "            if type(m) is Parameter:\n",
    "                ret.append(m)\n",
    "            elif isinstance(m, Module) or isinstance(m, ModuleList):\n",
    "                ret.extend(m.parameters())\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9688649-c900-4f00-bd06-38d93c1249cf",
   "metadata": {},
   "source": [
    "# Initialization and optimization\n",
    "\n",
    "**Question 1:** Implement the different initialisation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "337e899b-7551-42f5-ba48-bfef526f86a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(b):\n",
    "    b[:] = 0.\n",
    "\n",
    "def glorot_init(W):\n",
    "    '''inplace initialiase with glorot method'''\n",
    "    W[:] = np.random.uniform(-np.sqrt(6. / (W.shape[0] + W.shape[1])), \n",
    "                             np.sqrt(6. / (W.shape[0] + W.shape[1])), \n",
    "                             W.shape)\n",
    "    return W\n",
    "\n",
    "    raise NotImplementedError\n",
    "# Look at slides for the formula!\n",
    "def kaiming_init(W):\n",
    "    W[:] = np.random.uniform(-np.sqrt(6. / W.shape[0]), \n",
    "                             np.sqrt(6. / W.shape[0]), \n",
    "                             W.shape)\n",
    "    return W\n",
    "    raise NotImplementedError('Implement the initialization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ee98a-e90a-4ddb-b575-1688371efc05",
   "metadata": {},
   "source": [
    "We will implement the Stochastic gradient descent through an object, in the init function this object will store the different parameters (in a list format). The step function will update the parameters (see slides), notice that the gradient is stored in the nodes (grad attribute). Finally it will be necessary after each update to reset all the gradient to zero (in the method zero_grad) because we do not want to accumumlate gradient of all previous step.\n",
    "\n",
    "**Question 2:** Implement the SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbb7d09f-f4ae-472f-8b6a-f39fbaf8dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple gradient descent optimizer\n",
    "class SGD:\n",
    "    def __init__(self, params, lr=0.1):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        # raise NotImplementedError\n",
    "        \n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            p.value -= self.lr * p.grad\n",
    "        # raise NotImplementedError\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.zero_grad()\n",
    "        # raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76263efd-aed5-4a9a-b954-d9df83c9e8f4",
   "metadata": {},
   "source": [
    "# Networks and training loop\n",
    "\n",
    "We first create a simple linear classifier, similar to the first lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50268482-0561-4e42-a85d-d7f592ca9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNetwork(Module):\n",
    "    def __init__(self, dim_input, dim_output):\n",
    "        # build the parameters\n",
    "        self.W = Parameter(np.ndarray((dim_output, dim_input)), name=\"W\")\n",
    "        self.b = Parameter(np.ndarray((dim_output,)), name=\"b\")\n",
    "\n",
    "        self.init_parameters()\n",
    "        # raise NotImplementedError\n",
    "        \n",
    "    def init_parameters(self):\n",
    "        # init parameters of the network (i.e W and b)\n",
    "        glorot_init(self.W.value)\n",
    "        zero_init(self.b.value)\n",
    "        # raise NotImplementedError\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # perform the forward pass\n",
    "        return affine_transform()(self.W, self.b, x)\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49500117-74d1-4012-9096-53e3b8f298fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55d9ac22-7355-4d0f-828c-7a14c34c42dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30770062, -0.12891701,  0.0547308 , -0.27474205,  0.29604521,\n",
       "       -0.23439255,  0.05204224, -0.02639815, -0.12856254, -0.29513114])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# those lines should be executed correctly\n",
    "lin1 = LinearNetwork(784, 10)\n",
    "lin2 = LinearNetwork(10, 5)\n",
    "x = ComputationGraphNode(train_data[0][0])\n",
    "a = lin1.forward(x + x)\n",
    "b = TanH()(a)\n",
    "c = lin2.forward(b)\n",
    "\n",
    "c.backward()\n",
    "x.grad[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b40f7e-5c56-4c49-bcc7-82b27730e3b6",
   "metadata": {},
   "source": [
    "We will train several neural networks.\n",
    "Therefore, we encapsulate the training loop in a function.\n",
    "\n",
    "**warning**: you have to call optimizer.zero_grad() before each backward pass to reinitialize the gradient of the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "557d5a9a-c5b3-455b-9084-20470d1901db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(network, optimizer, train_data, dev_data, n_epochs=10):\n",
    "    for epoch in range(n_epochs):\n",
    "        # Initialize metrics for training and validation\n",
    "        total_train_loss, correct_train_preds = 0, 0\n",
    "        total_val_loss, correct_val_preds = 0, 0\n",
    "        \n",
    "        # Training Phase\n",
    "        for x_train, y_train in zip(*train_data):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions_train = network.forward(ComputationGraphNode(x_train))\n",
    "            loss = nll()(predictions_train, ComputationGraphNode(y_train))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.value.mean()\n",
    "            correct_train_preds += (np.argmax(predictions_train.value, axis=0) == y_train).sum()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_data[0].shape[0]\n",
    "\n",
    "        # Validation Phase\n",
    "        for x_val, y_val in zip(*dev_data):\n",
    "            predictions_val = network.forward(ComputationGraphNode(x_val))\n",
    "            loss_val = nll()(predictions_val, ComputationGraphNode(y_val))\n",
    "\n",
    "            total_val_loss += loss_val.value\n",
    "            correct_val_preds += (np.argmax(predictions_val.value, axis=0) == y_val).sum()\n",
    "\n",
    "        val_accuracy = correct_val_preds / dev_data[1].shape[0]\n",
    "        avg_val_loss = total_val_loss / dev_data[0].shape[0]\n",
    "\n",
    "        # Output the specified metrics: mean training loss and validation accuracy\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: mean loss -> {avg_train_loss:.4f}, validation accuracy -> {val_accuracy:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d3e4305-1736-4987-a5d6-d0902dc78738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: mean loss -> 0.3753, validation accuracy -> 0.920100\n",
      "Epoch 2/10: mean loss -> 0.3098, validation accuracy -> 0.925900\n",
      "Epoch 3/10: mean loss -> 0.2975, validation accuracy -> 0.925900\n",
      "Epoch 4/10: mean loss -> 0.2906, validation accuracy -> 0.926000\n",
      "Epoch 5/10: mean loss -> 0.2858, validation accuracy -> 0.926700\n",
      "Epoch 6/10: mean loss -> 0.2822, validation accuracy -> 0.926700\n",
      "Epoch 7/10: mean loss -> 0.2793, validation accuracy -> 0.927000\n",
      "Epoch 8/10: mean loss -> 0.2769, validation accuracy -> 0.927300\n",
      "Epoch 9/10: mean loss -> 0.2748, validation accuracy -> 0.927300\n",
      "Epoch 10/10: mean loss -> 0.2730, validation accuracy -> 0.927600\n"
     ]
    }
   ],
   "source": [
    "dim_input = 28*28\n",
    "dim_output = 10\n",
    "\n",
    "network = LinearNetwork(dim_input, dim_output)\n",
    "optimizer = SGD(network.parameters(), 0.01)\n",
    "\n",
    "training_loop(network, optimizer, train_data, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab8c45-1492-4be0-a3bd-d257066b5876",
   "metadata": {},
   "source": [
    "After you finished the linear network, you can move to a deep network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3180e14e-6490-4008-8fda-a1f7f7f71bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork(Module):\n",
    "    def __init__(self, dim_input, dim_output, hidden_dim, n_layers, tanh=False):\n",
    "        \n",
    "        self.layers = ModuleList()\n",
    "        self.tanh = tanh\n",
    "        self.n_layers = n_layers\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_output = dim_output\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "\n",
    "        # Initialize layers\n",
    "        # Input to first hidden layer\n",
    "        self.layers.append(LinearNetwork(dim_input, hidden_dim))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(1, n_layers):\n",
    "            self.layers.append(LinearNetwork(hidden_dim, hidden_dim))\n",
    "\n",
    "        # Last hidden to output layer\n",
    "        self.layers.append(LinearNetwork(hidden_dim, dim_output))\n",
    "\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.init_parameters()\n",
    "        \n",
    "    def init_parameters(self):\n",
    "        # Initialize parameters for each layer\n",
    "        for layer in self.layers:\n",
    "            layer.init_parameters()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        '''Forward pass of the network'''\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i].forward(x)\n",
    "            if self.tanh:\n",
    "                x = TanH()(x)\n",
    "            else:\n",
    "                x = ReLU()(x)\n",
    "        x = self.layers[-1].forward(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54b2e256-b2fe-4e7f-8f5d-777ba54cc7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: mean loss -> 0.2433, validation accuracy -> 0.962100\n",
      "Epoch 2/5: mean loss -> 0.1128, validation accuracy -> 0.968100\n",
      "Epoch 3/5: mean loss -> 0.0798, validation accuracy -> 0.959900\n",
      "Epoch 4/5: mean loss -> 0.0611, validation accuracy -> 0.967400\n",
      "Epoch 5/5: mean loss -> 0.0476, validation accuracy -> 0.966200\n"
     ]
    }
   ],
   "source": [
    "dim_input = 28*28\n",
    "dim_output = 10\n",
    "\n",
    "network = DeepNetwork(dim_input, dim_output, 100, 2)\n",
    "optimizer = SGD(network.parameters(), 0.01)\n",
    "\n",
    "training_loop(network, optimizer, train_data, dev_data, n_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af7721-8e42-4b4f-aa58-fe3b02d3d867",
   "metadata": {},
   "source": [
    "## Better Optimizer\n",
    "Implement the SGD with momentum, notice that you will need to store the cumulated gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b804a3fc-9136-4d2f-8f18-a394821401de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDWithMomentum:\n",
    "    def __init__(self, params, lr=0.1, momentum=0.5):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.velocities = [np.zeros_like(p.value) for p in self.params] # Initialize velocities to zero\n",
    "\n",
    "    def step(self):\n",
    "        for i, (p, v) in enumerate(zip(self.params, self.velocities)):\n",
    "            if p.grad is not None:  # Check if gradients are computed\n",
    "                self.velocities[i] = self.momentum * v - self.lr * p.grad # Update velocity \n",
    "                p.value += self.velocities[i]  # Update parameter using gradients and velocities\n",
    "            else:\n",
    "                print(f\"No gradient for parameter {p.name}\")\n",
    "\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45ccc31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: mean loss -> 0.2923, validation accuracy -> 0.955400\n",
      "Epoch 2/10: mean loss -> 0.1518, validation accuracy -> 0.962500\n",
      "Epoch 3/10: mean loss -> 0.1168, validation accuracy -> 0.966400\n",
      "Epoch 4/10: mean loss -> 0.0956, validation accuracy -> 0.958600\n",
      "Epoch 5/10: mean loss -> 0.0850, validation accuracy -> 0.966300\n",
      "Epoch 6/10: mean loss -> 0.0772, validation accuracy -> 0.964000\n",
      "Epoch 7/10: mean loss -> 0.0681, validation accuracy -> 0.967200\n",
      "Epoch 8/10: mean loss -> 0.0647, validation accuracy -> 0.964900\n",
      "Epoch 9/10: mean loss -> 0.0716, validation accuracy -> 0.968600\n",
      "Epoch 10/10: mean loss -> 0.0579, validation accuracy -> 0.970600\n"
     ]
    }
   ],
   "source": [
    "dim_input = 28*28\n",
    "dim_output = 10\n",
    "\n",
    "network = DeepNetwork(dim_input, dim_output, 100, 3, tanh=False)\n",
    "optimizer = SGDWithMomentum(network.parameters(), 0.01, momentum=0.4)\n",
    "\n",
    "training_loop(network, optimizer, train_data, dev_data, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8739e57-a82b-4908-9488-1b83d6e9a51f",
   "metadata": {},
   "source": [
    "## Bonus: Batch SGD\n",
    "Propose a methods to take into account batch of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824876ff-1dca-44ec-a777-620dfdd0d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch_sgd(model, optimizer, train_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()  # éç½®æ¢¯åº¦\n",
    "            outputs = model.forward(inputs)  # ååä¼ æ­\n",
    "            loss = compute_loss(outputs, targets)  # è®¡ç®æå¤±\n",
    "            loss.backward()  # ååä¼ æ­ï¼è®¡ç®æ¢¯åº¦\n",
    "            optimizer.step()  # æ´æ°æ¨¡ååæ°\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
