{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3710942-d267-47db-b6cf-15cce55fef50",
   "metadata": {},
   "source": [
    "# Neural network: first experiments with a linear model\n",
    "\n",
    "## student 1: Ying LAI, student 2: Yingjie LIU\n",
    "\n",
    "In this lab exercise we will code a neural network using numpy, without a neural network library.\n",
    "Next week, the lab exercise will be to extend this program with hidden layers and activation functions.\n",
    "\n",
    "The task is digit recognition: the neural network has to predict which digit in $\\{0...9\\}$ is written in the input picture. We will use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a standard benchmark in machine learning.\n",
    "\n",
    "The model is a simple linear  classifier $o = \\operatorname{softmax}(Wx + b)$ where:\n",
    "* $x$ is an input image that is represented as a column vector, each value being the \"color\" of a pixel\n",
    "* $W$ and $b$ are the parameters of the classifier\n",
    "* $\\operatorname{softmax}$ transforms the output weight (logits) into probabilities\n",
    "* $o$ is column vector that contains the probability of each category\n",
    "\n",
    "We will train this model via stochastic gradient descent by minimizing the negative log-likelihood of the data:\n",
    "$$\n",
    "    \\hat{W}, \\hat{b} = \\operatorname{argmin}_{W, b} \\sum_{x, y} - \\log p(y | x)\n",
    "$$\n",
    "Although this is a linear model, it classifies raw data without any manual feature extraction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "780e4c48-dd03-40d8-857c-759b997f9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs that we will use\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# To load the data we will use the script of Gaetan Marceau Caron\n",
    "# You can download it from the course webiste and move it to the same directory that contains this ipynb file\n",
    "import dataset_loader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "039010e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-26 20:50:56--  https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
      "正在解析主机 github.com (github.com)... 140.82.121.3\n",
      "正在连接 github.com (github.com)|140.82.121.3|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 302 Found\n",
      "位置：https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz [跟随至新的 URL]\n",
      "--2024-03-26 20:50:56--  https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\n",
      "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度：17051982 (16M) [application/octet-stream]\n",
      "正在保存至: “mnist.pkl.gz”\n",
      "\n",
      "mnist.pkl.gz        100%[===================>]  16.26M  22.0MB/s  用时 0.7s      \n",
      "\n",
      "2024-03-26 20:50:58 (22.0 MB/s) - 已保存 “mnist.pkl.gz” [17051982/17051982])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz -O mnist.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e162f3f-18cb-4fd3-b003-da463965f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mnist dataset \n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    # this link doesn't work any more,\n",
    "    # seach on google for the file \"mnist.pkl.gz\"\n",
    "    # and download it\n",
    "    !wget https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
    "\n",
    "# if you have it somewhere else, you can comment the lines above\n",
    "# and overwrite the path below\n",
    "mnist_path = \"./mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fca3e04-965a-44b1-9419-0b532c3352b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 3 splits\n",
    "train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371bc90-6323-4109-8419-93f6fa309cae",
   "metadata": {},
   "source": [
    "Each dataset is a list with two elemets:\n",
    "* data[0] contains images\n",
    "* data[1] contains labels\n",
    "\n",
    "Data is stored as numpy.ndarray. You can use data[0][i] to retrieve image number i and data[1][i] to retrieve its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22acb142-3f16-4bb4-9704-1b243f27df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x117840940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 900\n",
    "label = train_data[1][index]\n",
    "picture = train_data[0][index]\n",
    "\n",
    "print(\"label: %i\" % label)\n",
    "plt.imshow(picture.reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d12ddbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([6, 0, 0, ..., 7, 9, 8])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#显示一下数据集的构成 方便理解 show the srcture of dataset   #数据集和他的标签\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65aa50a-ea30-4708-b9ca-7dae904687f0",
   "metadata": {},
   "source": [
    "**Question 1:** What are the characteristics of training data? (number of samples, dimension of input, number of labels)\n",
    "\n",
    "The documentation of ndarray class is available here: https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91efaaab-2287-417d-9a62-b4a631b0b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDimDataset(data):\n",
    "    n_training = data[0].shape[0]\n",
    "    n_feature = data[0].shape[1]\n",
    "    n_label = len(np.unique(data[1]))\n",
    "    # raise NotImplementedError('Implement the function')\n",
    "    return n_training, n_feature, n_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8938454-4727-41c7-802c-3955fba9f7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDimDataset(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da32ee-fa5c-48c8-af0f-580eba03f6c5",
   "metadata": {},
   "source": [
    "# 1. Building functions\n",
    "\n",
    "We now need to build functions that are required for the neural network.\n",
    "$$\n",
    "    o = \\operatorname{softmax}(Wx + b) \\\\\n",
    "    L(x, y) = -\\log p(y | x) = -\\log o[y]\n",
    "$$\n",
    "\n",
    "Note that in numpy, operator @ is used for matrix multiplication while * is used for element-wise multiplication.\n",
    "The documentation for linear algebra in numpy is available here: https://docs.scipy.org/doc/numpy/reference/routines.linalg.html\n",
    "\n",
    "The first operation is the affine transformation $v = Wx + b$.\n",
    "To compute the gradient, it is often convenient to write the forward pass as $v[i] = b[i] + \\sum_j W[i, j] x[j]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4989ef8-ec12-487a-988b-a3d657d454a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-4.25801879],\n",
       "       [-0.51341195],\n",
       "       [ 0.30829108],\n",
       "       [-0.04793931],\n",
       "       [-2.91195036],\n",
       "       [-2.02153797],\n",
       "       [-0.54927267],\n",
       "       [-0.55614338],\n",
       "       [ 1.08235921],\n",
       "       [-0.04459657]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.randn(5,10)\n",
    "x = np.random.randn(1,5)\n",
    "print(W.transpose(1,0).shape)\n",
    "np.matmul(W.transpose(1,0), x.transpose(1,0)) # 转置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c167d320-9a24-43de-9311-0846e2f1a280",
   "metadata": {},
   "source": [
    "**Question 2:**  Complete the two functions `affine_transform` and `backward_affine_transform`. The last function compute the gradient the loss function according to weights of the linear module. The gradient of the loss according to output of the linear module is given as last parameter of the function backward_affine_transform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee5da951-0405-4aaa-9207-b02aff3116cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - W: projection matrix\n",
    "# - b: bias\n",
    "# - x: input features\n",
    "# Output:\n",
    "# - vector\n",
    "def affine_transform(W, b, x):\n",
    "    return np.matmul(W, x) + b\n",
    "    raise NotImplementedError('Implement the function')\n",
    "# Input:\n",
    "# - W: projection matrix\n",
    "# - b: bias\n",
    "# - x: input features\n",
    "# - g: incoming gradient\n",
    "# Output:\n",
    "# - g_W: gradient wrt W\n",
    "# - g_b: gradient wrt b\n",
    "def backward_affine_transform(W, b, x, g):\n",
    "    g_W = np.dot(g.reshape(len(g),1),x.reshape(1,len(x)))\n",
    "    g_b = g\n",
    "    # raise NotImplementedError('Implement the function')\n",
    "    return g_W, g_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80dfc98c-5ec4-42ac-9d5d-dc671eec3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.asarray([[ 0.63024213,  0.53679375, -0.92079597],\n",
    " [-0.1155045,   0.62780356, -0.67961305],\n",
    " [ 0.08465286, -0.06561815, -0.39778322],\n",
    " [ 0.8242268,   0.58907262, -0.52208052],\n",
    " [-0.43894227, -0.56993247,  0.09520727]])\n",
    "\n",
    "\n",
    "b = np.asarray([ 0.42706842,  0.69636598, -0.85611933, -0.08682553,  0.83160079])\n",
    "x = np.asarray([-0.32809223, -0.54751413,  0.81949319])\n",
    "\n",
    "o_gold = np.asarray([-0.82819732, -0.16640748, -1.17394705, -1.10761496,  1.36568213])\n",
    "g = np.asarray([-0.08938868,  0.44083873, -0.2260743,  -0.96196726, -0.53428805])\n",
    "g_W_gold = np.asarray([[ 0.02932773,  0.04894156, -0.07325341],\n",
    " [-0.14463576, -0.24136543,  0.36126434],\n",
    " [ 0.07417322,  0.12377887, -0.18526635],\n",
    " [ 0.31561399,  0.52669067, -0.78832562],\n",
    " [ 0.17529576,  0.29253025, -0.43784542]])\n",
    "g_b_gold = np.asarray([-0.08938868,  0.44083873, -0.2260743,  -0.96196726, -0.53428805])\n",
    "\n",
    "\n",
    "# quick test of the forward pass\n",
    "o = affine_transform(W, b, x)\n",
    "if o.shape != o_gold.shape:\n",
    "    raise RuntimeError(\"Unexpected output dimension: got %s, expected %s\" % (str(o.shape), str(o_gold.shape)))\n",
    "if not np.allclose(o, o_gold):\n",
    "    raise RuntimeError(\"Output of the affine_transform function is incorrect\")\n",
    "    \n",
    "# quick test if the backward pass\n",
    "g_W, g_b = backward_affine_transform(W, b, x, g)\n",
    "if g_W.shape != g_W_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension for W: got %s, expected %s\" % (str(g_W.shape), str(g_W_gold.shape)))\n",
    "if g_b.shape != g_b_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension for b: got %s, expected %s\" % (str(g_b.shape), str(g_b_gold.shape)))\n",
    "if not np.allclose(g_W, g_W_gold):\n",
    "    raise RuntimeError(\"Gradient of W is incorrect\")\n",
    "if not np.allclose(g_b, g_b_gold):\n",
    "    raise RuntimeError(\"Gradient of b is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed21a0ba-a70e-4d81-a2ef-ffff48ca7b29",
   "metadata": {},
   "source": [
    "The softmax function:\n",
    "$$\n",
    "     o = \\operatorname{softmax}(w)\n",
    "$$\n",
    "where $w$ is a vector of logits in $\\mathbb R$ and $o$ a vector of probabilities such that:\n",
    "$$\n",
    "    o[i] = \\frac{\\exp(w[i])}{\\sum_j \\exp(w[j])}\n",
    "$$\n",
    "We do not need to implement the backward for this experiment.\n",
    "\n",
    "**Question 3** Implement the function softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3079bfae-61fd-4bb3-aa5b-5e8f560334cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - x: vector of logits\n",
    "# Output\n",
    "# - vector of probabilities\n",
    "def softmax(x):\n",
    "    b = np.max(x, axis=0, keepdims=True)\n",
    "    e_x = np.exp(x - b)\n",
    "    return e_x /np.sum(e_x, axis=0, keepdims=True)\n",
    "    raise NotImplementedError('Implement the function')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b72da73-9779-4374-9fdc-16e35e0181e9",
   "metadata": {},
   "source": [
    "**WARNING:** is your implementation numerically stable?\n",
    "\n",
    "The $\\exp$ function results in computations that overflows (i.e. results in numbers that cannot be represented with floating point numbers).\n",
    "Therefore, it is always convenient to use the following trick to improve stability: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b75087-b8bd-4d9c-bc6a-65f4d68114ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Example for testing the numerical stability of softmax\n",
    "# It should return [1., 0. ,0.], not [nan, 0., 0.]\n",
    "z = [1000000,1,100]\n",
    "print(softmax(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55067f3f-c6cf-408a-a574-ae805bb804e8",
   "metadata": {},
   "source": [
    "**Question 4**: From the result of the cell above, what can you say about the softmax output, even when it is stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04b0495c-9cc4-4877-a14b-1b3a170661a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57467369, 0.09053556, 0.10808234, 0.09486917, 0.13183925])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just too simple test for the softmax function\n",
    "x = np.asarray([0.92424884, -0.92381088, -0.74666024, -0.87705478, -0.54797015])\n",
    "y_gold = np.asarray([0.57467369, 0.09053556, 0.10808233, 0.09486917, 0.13183925])\n",
    "\n",
    "y = softmax(x)\n",
    "if not np.allclose(y, y_gold):\n",
    "    raise RuntimeError(\"Output of the softmax function is incorrect\")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aabf403-e4a2-4acb-a835-0ae5b7d7a2b4",
   "metadata": {},
   "source": [
    "Finally, we build the loss function and its gradient for training the network.\n",
    "\n",
    "The loss function is the negative log-likelihood defined as:\n",
    "$$\n",
    "    \\mathcal L(x, gold) = -\\log \\frac{\\exp(x[gold])}{\\sum_j \\exp(x[j])} = -x[gold] + \\log \\sum_j \\exp(x[j])\n",
    "$$\n",
    "This function is also called the cross-entropy loss (in Pytorch, different names are used dependending if the inputs are probabilities or raw logits).\n",
    "\n",
    "Similarly to the softmax, we have to rely on the log-sum-exp trick to stabilize the computation: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "\n",
    "**Question 5:** Implement the forward and backward function for the negative loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14cd59b9-7a7f-446a-81fa-8eaf3abf6de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - x: vector of logits\n",
    "# - gold: index of the gold class\n",
    "# Output:\n",
    "# - scalare equal to -log(softmax(x)[gold])\n",
    "def nll(x, gold):\n",
    "    return -x[gold]+np.log(sum(np.exp(x)))\n",
    "    raise NotImplementedError('Implement the function')\n",
    "\n",
    "# Input:\n",
    "# - x: vector of logits\n",
    "# - gold: index of the gold class\n",
    "# - gradient (scalar)\n",
    "# Output:\n",
    "# - gradient wrt x\n",
    "def backward_nll(x, gold, g):\n",
    "    g_x = softmax(x)\n",
    "    g_x[gold] -= 1\n",
    "    g_x *= g\n",
    "    # raise NotImplementedError('Implement the function')\n",
    "    return g_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54b9ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "x = np.asarray([-0.13590009, -0.83649656,  0.03130881,  0.42559402,  0.08488182])\n",
    "y_gold = 1.5695014420179738\n",
    "g_gold = np.asarray([ 0.17609875,  0.08739591, -0.79185107,  0.30875221,  0.2196042 ])\n",
    "\n",
    "y = nll(x, 2)\n",
    "g = backward_nll(x, 2, 1.)\n",
    "\n",
    "\n",
    "if not np.allclose(y, y_gold):\n",
    "    raise RuntimeError(\"Output is incorrect\")\n",
    "\n",
    "if g.shape != g_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension: got %s, expected %s\" % (str(g.shape), str(g_gold.shape)))\n",
    "if not np.allclose(g, g_gold):\n",
    "    raise RuntimeError(\"Gradient is incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9da38519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5695014476527018"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdf49d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17609875,  0.08739591, -0.79185107,  0.30875221,  0.2196042 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0d56d-82ea-478c-a72e-042e5bc63240",
   "metadata": {},
   "source": [
    "The following code test the implementation of the gradient using finite-difference approximation, see: https://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/\n",
    "\n",
    "Your implementation should pass this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f66cd42-9cae-496d-84df-5deaf41817c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is python re-implementation of the test from the Dynet library\n",
    "# https://github.com/clab/dynet/blob/master/dynet/grad-check.cc\n",
    "\n",
    "def is_almost_equal(grad, computed_grad):\n",
    "    #print(grad, computed_grad)\n",
    "    f = abs(grad - computed_grad)\n",
    "    m = max(abs(grad), abs(computed_grad))\n",
    "\n",
    "    if f > 0.01 and m > 0.:\n",
    "        f /= m\n",
    "\n",
    "    if f > 0.01 or math.isnan(f):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def check_gradient(function, weights, true_grad, alpha = 1e-3):\n",
    "    # because input can be of any dimension,\n",
    "    # we build a view of the underlying data with the .shape(-1) method\n",
    "    # then we can access any element of the tensor as a elements of a list\n",
    "    # with a single dimension\n",
    "    weights_view = weights.reshape(-1)\n",
    "    true_grad_view = true_grad.reshape(-1)\n",
    "    for i in range(weights_view.shape[0]):\n",
    "        old = weights_view[i]\n",
    "\n",
    "        weights_view[i] = old - alpha\n",
    "        value_left = function(weights).reshape(-1)\n",
    "\n",
    "        weights_view[i] = old + alpha\n",
    "        value_right = function(weights).reshape(-1)\n",
    "\n",
    "        weights_view[i] = old\n",
    "        grad = (value_right - value_left) / (2. * alpha)\n",
    "\n",
    "        if not is_almost_equal(grad, true_grad_view[i]):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f5/jhrqh_cs6p72_tsw42gjd4cc0000gn/T/ipykernel_60972/199068466.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if f > 0.01 or math.isnan(f):\n"
     ]
    }
   ],
   "source": [
    "# Test the affine transformation\n",
    "\n",
    "x = np.random.uniform(-1, 1, (6,))\n",
    "W = np.random.uniform(-1, 1, (3, 6))\n",
    "b = np.random.uniform(-1, 1, (3,))\n",
    "\n",
    "for i in range(3):\n",
    "    y = affine_transform(W, b, x)\n",
    "    g = np.zeros_like(y)\n",
    "    g[i] = 1.\n",
    "    g_W, _ = backward_affine_transform(W, b, x, g)\n",
    "    print(check_gradient(lambda W: affine_transform(W, b, x)[i], W, g_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3858f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f5/jhrqh_cs6p72_tsw42gjd4cc0000gn/T/ipykernel_60972/199068466.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  if f > 0.01 or math.isnan(f):\n"
     ]
    }
   ],
   "source": [
    "# Test the negative likelihood loss\n",
    "\n",
    "x = np.random.uniform(-1, 1, (4,))\n",
    "\n",
    "for gold in range(4):\n",
    "    y = nll(x, gold)\n",
    "    g = np.zeros_like(x)\n",
    "    g_gold = backward_nll(x, gold, 1.)\n",
    "    print(check_gradient(lambda x: nll(x, gold), x, g_gold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e74c83-d581-4499-acb5-6b6ac57de0f7",
   "metadata": {},
   "source": [
    "# 2. Parameter initialization\n",
    "\n",
    "We are now going to build the function that will be used to initialize the parameters of the neural network before training.\n",
    "Note that for parameter initialization you must use **in-place** operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b81523ec-c37d-4061-ad8c-59956121eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random ndarray\n",
    "a = np.random.uniform(-1, 1, (5,))\n",
    "\n",
    "# this does not change the data of the ndarray created above!\n",
    "# it creates a new ndarray and replace the reference stored in a\n",
    "a = np.zeros((5, ))\n",
    "\n",
    "# this will change the underlying data of the ndarray that a points to\n",
    "a[:] = 0\n",
    "\n",
    "# similarly, this creates a new array and change the object pointed by a\n",
    "a = a + 1\n",
    "\n",
    "# while this change the underlying data of a\n",
    "a += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a932f883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc53dc-2ca1-4619-a0e2-bdce3f65ee1d",
   "metadata": {},
   "source": [
    "For an affine transformation, it is common to:\n",
    "* initialize the bias to 0\n",
    "* initialize the projection matrix with Glorot initialization (also known as Xavier initialization)\n",
    "\n",
    "The formula for Glorot initialization can be found in equation 16 (page 5) of the original paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "\n",
    "**Question 6:** Fill the two initilization functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5171ab4-848a-47f0-92db-81acb31aeabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(b):\n",
    "    return np.zeros(b)\n",
    "    raise NotImplementedError('Implement the function')\n",
    "\n",
    "def glorot_init(W):\n",
    "    dim_output, dim_input = W.shape\n",
    "    limit = np.sqrt(6 / (dim_input + dim_output))\n",
    "    return np.random.uniform(-limit, limit, (dim_output, dim_input))\n",
    "    raise NotImplementedError('Implement the function')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1aede1-96af-4f2c-97ec-273b95a256c8",
   "metadata": {},
   "source": [
    "# 3. Building and training the neural network\n",
    "\n",
    "In our simple example, creating the neural network is simply instantiating the parameters $W$ and $b$.\n",
    "They must be ndarray object with the correct dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc4a37-d975-45cb-b225-f22ab642ca4b",
   "metadata": {},
   "source": [
    "**Question 7:** Fill the function that create and initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fad7abf3-fe48-4738-9eb3-2b716cb11cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameters(dim_input, dim_output):\n",
    "    W = np.zeros((dim_output, dim_input))\n",
    "    W = glorot_init(W)\n",
    "    b = zero_init(dim_output)\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b83f22-cc7d-4c05-9607-8d8c809558cc",
   "metadata": {},
   "source": [
    "The recent success of deep learning is (partly) due to the ability to train very big neural networks.\n",
    "However, researchers became interested in building small neural networks to improve computational efficiency and memory usage.\n",
    "Therefore, we often want to compare neural networks by their number of parameters, i.e. the size of the memory required to store the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e4375-f265-4bb2-9f37-889713f7e277",
   "metadata": {},
   "source": [
    "**Question 8:** Fill the function that  print the number of parameters of the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7729a857-04ab-4a7a-b85c-cf171b62501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_parameters(W, b):\n",
    "    n = W.size + b.size\n",
    "    print(\"Number of parameters: %i\" % (n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce568ce-415c-4750-9169-5c67dc9653cf",
   "metadata": {},
   "source": [
    "We can now create the neural network and print its number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8f54c2a-98a1-4c06-94dc-511b05cd7bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 7850\n"
     ]
    }
   ],
   "source": [
    "dim_input = len(train_data[0][0])\n",
    "dim_output = len(np.unique(train_data[1]))\n",
    "W, b = create_parameters(dim_input, dim_output)\n",
    "print_n_parameters(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99f7f13-4f6d-4c03-a9bf-5776571dec58",
   "metadata": {},
   "source": [
    "Finally, the training loop!\n",
    "\n",
    "The training loop should be structured as follows:\n",
    "* we do **epochs** over the data, i.e. one epoch is one loop over the dataset\n",
    "* at each epoch, we first loop over the data and update the network parameters with respect to the loss gradient\n",
    "* at the end of each epoch, we evaluate the network on the dev dataset\n",
    "* after all epochs are done, we evaluate our network on the test dataset and compare its performance with the performance on dev\n",
    "\n",
    "During training, it is useful to print the following information:\n",
    "* the mean loss over the epoch: it should be decreasing!\n",
    "* the accuracy on the dev set: it should be increasing!\n",
    "* the accuracy on the train set: it shoud be increasing!\n",
    "\n",
    "If you observe a decreasing loss (+increasing accuracy on test data) but decreasing accuracy on dev data, your network is overfitting!\n",
    "\n",
    "Once you have build **and tested** this a simple training loop, you should introduce the following improvements:\n",
    "* instead of evaluating on dev after each loop on the training data, you can also evaluate on dev n times per epoch\n",
    "* shuffle the data before each epoch\n",
    "* instead of memorizing the parameters of the last epoch only, you should have a copy of the parameters that produced the best value on dev data during training and evaluate on test with those instead of the parameters after the last epoch\n",
    "* learning rate decay: if you do not observe improvement on dev, you can try to reduce the step size\n",
    "\n",
    "After you conducted (successful?) experiments, you should write a report with results (in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55b5b6ae-43be-4017-8a19-9cbd122302f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.377546, accuracy = 0.892740, dev_loss = 0.290028, dev_accuracy = 0.917000\n",
      "Epoch 1: loss = 0.310318, accuracy = 0.912540, dev_loss = 0.279069, dev_accuracy = 0.922200\n",
      "Epoch 2: loss = 0.297887, accuracy = 0.915680, dev_loss = 0.275319, dev_accuracy = 0.924200\n",
      "Epoch 3: loss = 0.290829, accuracy = 0.918220, dev_loss = 0.273491, dev_accuracy = 0.925200\n",
      "Epoch 4: loss = 0.285963, accuracy = 0.919720, dev_loss = 0.272447, dev_accuracy = 0.925500\n",
      "Epoch 5: loss = 0.282272, accuracy = 0.920820, dev_loss = 0.271819, dev_accuracy = 0.925600\n",
      "Epoch 6: loss = 0.279312, accuracy = 0.921760, dev_loss = 0.271450, dev_accuracy = 0.926400\n",
      "Epoch 7: loss = 0.276850, accuracy = 0.922600, dev_loss = 0.271257, dev_accuracy = 0.926600\n",
      "Epoch 8: loss = 0.274749, accuracy = 0.923120, dev_loss = 0.271192, dev_accuracy = 0.926500\n",
      "Epoch 9: loss = 0.272922, accuracy = 0.923800, dev_loss = 0.271221, dev_accuracy = 0.926800\n",
      "Epoch 10: loss = 0.271311, accuracy = 0.924140, dev_loss = 0.271323, dev_accuracy = 0.926900\n",
      "Epoch 11: loss = 0.269874, accuracy = 0.924500, dev_loss = 0.271480, dev_accuracy = 0.927100\n",
      "Epoch 12: loss = 0.268579, accuracy = 0.925080, dev_loss = 0.271680, dev_accuracy = 0.927700\n",
      "Epoch 13: loss = 0.267403, accuracy = 0.925520, dev_loss = 0.271913, dev_accuracy = 0.927400\n",
      "Epoch 14: loss = 0.266328, accuracy = 0.925840, dev_loss = 0.272172, dev_accuracy = 0.927400\n",
      "Epoch 15: loss = 0.265340, accuracy = 0.926180, dev_loss = 0.272450, dev_accuracy = 0.927700\n",
      "Epoch 16: loss = 0.264427, accuracy = 0.926280, dev_loss = 0.272744, dev_accuracy = 0.927900\n",
      "Epoch 17: loss = 0.263579, accuracy = 0.926500, dev_loss = 0.273049, dev_accuracy = 0.927600\n",
      "Epoch 18: loss = 0.262789, accuracy = 0.926760, dev_loss = 0.273361, dev_accuracy = 0.927500\n",
      "Epoch 19: loss = 0.262050, accuracy = 0.926980, dev_loss = 0.273679, dev_accuracy = 0.927300\n",
      "Test: loss = 0.286950, accuracy = 0.919200\n"
     ]
    }
   ],
   "source": [
    "# before training, we initialize the parameters of the network\n",
    "# zero_init(b)\n",
    "# glorot_init(W)\n",
    "\n",
    "n_epochs = 20 # number of epochs\n",
    "step = 0.01 # step size for gradient updates\n",
    "\n",
    "losses = np.zeros(n_epochs)\n",
    "accuracy = np.zeros(n_epochs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    # TODO\n",
    "    for i in range(len(train_data[0])):\n",
    "        x = train_data[0][i]\n",
    "        gold = train_data[1][i]\n",
    "        \n",
    "        # forward\n",
    "        y = affine_transform(W, b, x)\n",
    "        loss = nll(y, gold)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # check if the prediction is correct\n",
    "        if np.argmax(y) == gold:\n",
    "            epoch_accuracy += 1\n",
    "            accuracy[epoch] += 1\n",
    "        \n",
    "        # backward\n",
    "        g = backward_nll(y, gold, 1.)\n",
    "        g_W, g_b = backward_affine_transform(W, b, x, g)\n",
    "        \n",
    "        # gradient update\n",
    "        W -= step * g_W\n",
    "        b -= step * g_b\n",
    "\n",
    "   \n",
    "    losses[epoch] = epoch_loss / len(train_data[0])\n",
    "    accuracy[epoch] /= len(train_data[0])\n",
    "\n",
    "    dev_loss = 0\n",
    "    dev_accuracy = 0\n",
    "    for i in range(len(dev_data[0])):\n",
    "        x = dev_data[0][i]\n",
    "        gold = dev_data[1][i]\n",
    "    \n",
    "        y = affine_transform(W, b, x)\n",
    "        loss = nll(y, gold)\n",
    "        dev_loss += loss\n",
    "    \n",
    "        if np.argmax(y) == gold:\n",
    "            dev_accuracy += 1\n",
    "        \n",
    "    dev_loss /= len(dev_data[0])\n",
    "    dev_accuracy /= len(dev_data[0])\n",
    "\n",
    "    print(\"Epoch %i: loss = %f, accuracy = %f, dev_loss = %f, dev_accuracy = %f\" % (epoch, losses[epoch], accuracy[epoch], dev_loss, dev_accuracy))\n",
    "\n",
    "# Test evaluation\n",
    "# TODO\n",
    "test_loss = 0\n",
    "test_accuracy = 0\n",
    "for i in range(len(test_data[0])):\n",
    "    x = test_data[0][i]\n",
    "    gold = test_data[1][i]\n",
    "\n",
    "    y = affine_transform(W, b, x)\n",
    "    loss = nll(y, gold)\n",
    "    test_loss += loss\n",
    "\n",
    "    if np.argmax(y) == gold:\n",
    "        test_accuracy += 1\n",
    "\n",
    "test_loss /= len(test_data[0])\n",
    "test_accuracy /= len(test_data[0])\n",
    "\n",
    "print(\"Test: loss = %f, accuracy = %f\" % (test_loss, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118fce0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
